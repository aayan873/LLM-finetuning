{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf2322f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obliviontrek/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/obliviontrek/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.6.0+cu124\n",
      "PyTorch Lightning version: 2.5.5\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='.*overflowing tokens.*')\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader    \n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce8daf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration initialized\n",
      "Data path: /home/obliviontrek/Documents/llm-finetuning-kaggle\n",
      "Model: bert-base-uncased\n",
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    # Paths - Kaggle format\n",
    "    BASE_DIR = Path.cwd()\n",
    "    DATA_PATH = BASE_DIR\n",
    "    \n",
    "    # Model - Download directly from HuggingFace\n",
    "    MODEL_NAME = 'bert-base-uncased'\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LENGTH = 512\n",
    "    LEARNING_RATE = 5e-5\n",
    "    MAX_EPOCHS = 2\n",
    "    NUM_WORKERS = 4\n",
    "    HIDDEN_DIM = 256\n",
    "    NUM_CLASSES = 3\n",
    "    DROPOUT_RATE = 0.1\n",
    "    \n",
    "    # Other settings\n",
    "    SEED = 42\n",
    "    TEST_SIZE = 0.2\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Initialize configuration and set seed\n",
    "config = Config()\n",
    "set_seed(config.SEED)\n",
    "\n",
    "print(f\"Configuration initialized\")\n",
    "print(f\"Data path: {config.DATA_PATH}\")\n",
    "print(f\"Model: {config.MODEL_NAME}\")\n",
    "print(f\"Random seed: {config.SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab9bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (57477, 9)\n",
      "Training data columns: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
      "Training samples: 45981\n",
      "Model A wins: 16051\n",
      "Model B wins: 15721\n",
      "Ties: 14209\n",
      "Validation samples: 11496\n",
      "Model A wins: 4013\n",
      "Model B wins: 3931\n",
      "Ties: 3552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53973</th>\n",
       "      <td>4032881709</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>mixtral-8x7b-instruct-v0.1</td>\n",
       "      <td>[\"In a room I have only 3 sisters. Eve is read...</td>\n",
       "      <td>[\"Based on the information provided, Susan wou...</td>\n",
       "      <td>[\"The information about Susan's activities is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38792</th>\n",
       "      <td>2890889868</td>\n",
       "      <td>zephyr-7b-beta</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"\\/\\/ \\u67e5\\u627e\\u88ab\\u6295\\u7968\\u73a9\\u5...</td>\n",
       "      <td>[\"No, this code only checks if the targeted pl...</td>\n",
       "      <td>[\"\\u662f\\u7684\\uff0c\\u8fd9\\u6bb5\\u4ee3\\u7801\\u...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46462</th>\n",
       "      <td>3466836316</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>mixtral-8x7b-instruct-v0.1</td>\n",
       "      <td>[\"Hello there. Can you tell me what is the cur...</td>\n",
       "      <td>[\"Hello! The current capital of Brazil is Bras...</td>\n",
       "      <td>[\"The current capital of Brazil is Bras\\u00edl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16654</th>\n",
       "      <td>1243922820</td>\n",
       "      <td>zephyr-7b-beta</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>[\"can you predict the next year jee advanced p...</td>\n",
       "      <td>[\"I'm sorry but I'm not capable of predicting ...</td>\n",
       "      <td>[\"I'm sorry, but as an AI assistant, I do not ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45609</th>\n",
       "      <td>3399655443</td>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"how do turn the touchpad off in linux x11?\"]</td>\n",
       "      <td>[\"Here are a few ways to disable the touchpad ...</td>\n",
       "      <td>[\"Turning off the touchpad in Linux X11 enviro...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             model_a                     model_b  \\\n",
       "53973  4032881709  gpt-3.5-turbo-0125  mixtral-8x7b-instruct-v0.1   \n",
       "38792  2890889868      zephyr-7b-beta              mistral-medium   \n",
       "46462  3466836316  gpt-4-1106-preview  mixtral-8x7b-instruct-v0.1   \n",
       "16654  1243922820      zephyr-7b-beta          gpt-3.5-turbo-0613   \n",
       "45609  3399655443          claude-2.1                  gpt-4-0613   \n",
       "\n",
       "                                                  prompt  \\\n",
       "53973  [\"In a room I have only 3 sisters. Eve is read...   \n",
       "38792  [\"\\/\\/ \\u67e5\\u627e\\u88ab\\u6295\\u7968\\u73a9\\u5...   \n",
       "46462  [\"Hello there. Can you tell me what is the cur...   \n",
       "16654  [\"can you predict the next year jee advanced p...   \n",
       "45609     [\"how do turn the touchpad off in linux x11?\"]   \n",
       "\n",
       "                                              response_a  \\\n",
       "53973  [\"Based on the information provided, Susan wou...   \n",
       "38792  [\"No, this code only checks if the targeted pl...   \n",
       "46462  [\"Hello! The current capital of Brazil is Bras...   \n",
       "16654  [\"I'm sorry but I'm not capable of predicting ...   \n",
       "45609  [\"Here are a few ways to disable the touchpad ...   \n",
       "\n",
       "                                              response_b  winner_model_a  \\\n",
       "53973  [\"The information about Susan's activities is ...               0   \n",
       "38792  [\"\\u662f\\u7684\\uff0c\\u8fd9\\u6bb5\\u4ee3\\u7801\\u...               0   \n",
       "46462  [\"The current capital of Brazil is Bras\\u00edl...               0   \n",
       "16654  [\"I'm sorry, but as an AI assistant, I do not ...               1   \n",
       "45609  [\"Turning off the touchpad in Linux X11 enviro...               1   \n",
       "\n",
       "       winner_model_b  winner_tie  \n",
       "53973               0           1  \n",
       "38792               1           0  \n",
       "46462               0           1  \n",
       "16654               0           0  \n",
       "45609               0           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(config):\n",
    "    \n",
    "    train_file_path = config.DATA_PATH / 'train.csv'\n",
    "    if not train_file_path.exists():\n",
    "        raise FileNotFoundError(f\"Training data not found at {train_file_path}\")\n",
    "    \n",
    "    train_data = pd.read_csv(train_file_path)\n",
    "    print(f\"Training data shape: {train_data.shape}\")\n",
    "    print(f\"Training data columns: {train_data.columns.tolist()}\")\n",
    "    \n",
    "    train_data_split, validation_data = train_test_split(\n",
    "        train_data, \n",
    "        test_size=config.TEST_SIZE, \n",
    "        random_state=config.SEED, \n",
    "        stratify=train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].values\n",
    "    )\n",
    "    print(f\"Training samples: {len(train_data_split)}\")\n",
    "    \n",
    "\n",
    "    print(f\"Model A wins: {train_data_split['winner_model_a'].sum()}\")\n",
    "    print(f\"Model B wins: {train_data_split['winner_model_b'].sum()}\")\n",
    "    print(f\"Ties: {train_data_split['winner_tie'].sum()}\")\n",
    "    \n",
    "    print(f\"Validation samples: {len(validation_data)}\")\n",
    "    print(f\"Model A wins: {validation_data['winner_model_a'].sum()}\")\n",
    "    print(f\"Model B wins: {validation_data['winner_model_b'].sum()}\")\n",
    "    print(f\"Ties: {validation_data['winner_tie'].sum()}\")\n",
    "    \n",
    "    return train_data_split, validation_data\n",
    "\n",
    "# Load and explore the data\n",
    "train_df, val_df = load_data(config)\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d04b64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obliviontrek/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully!\n",
      "Vocabulary size: 30522\n",
      "Max length: 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tokenizer: {config.MODEL_NAME}\")\n",
    "tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "print(f\"Tokenizer loaded successfully!\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Max length: {config.MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c8bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Prepare the three text inputs\n",
    "        prompt = row['prompt']\n",
    "        model_a_response = row['response_a']\n",
    "        model_b_response = row['response_b']\n",
    "        \n",
    "        # Convert labels to single class\n",
    "        if row['winner_model_a'] == 1:\n",
    "            label = 0\n",
    "        elif row['winner_model_b'] == 1:\n",
    "            label = 1\n",
    "        else:  # winner_tie == 1\n",
    "            label = 2\n",
    "            \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'model_a_response': model_a_response,\n",
    "            'model_b_response': model_b_response,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    \"\"\"Test dataset for inference\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Prepare the three text inputs\n",
    "        prompt = row['prompt']\n",
    "        model_a_response = row['response_a']\n",
    "        model_b_response = row['response_b']\n",
    "            \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'model_a_response': model_a_response,\n",
    "            'model_b_response': model_b_response\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c99da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer, max_length=512):\n",
    "    \"\"\"Custom collate function for batch tokenization\"\"\"\n",
    "    prompts = [item['prompt'] for item in batch]\n",
    "    model_a_responses = [item['model_a_response'] for item in batch]\n",
    "    model_b_responses = [item['model_b_response'] for item in batch]\n",
    "    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    # Tokenize each input type\n",
    "    prompt_encoding = tokenizer(\n",
    "        prompts, padding=True, truncation=True, \n",
    "        max_length=max_length, return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    model_a_encoding = tokenizer(\n",
    "        model_a_responses, padding=True, truncation=True,\n",
    "        max_length=max_length, return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    model_b_encoding = tokenizer(\n",
    "        model_b_responses, padding=True, truncation=True,\n",
    "        max_length=max_length, return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'prompt_input_ids': prompt_encoding['input_ids'],\n",
    "        'prompt_attention_mask': prompt_encoding['attention_mask'],\n",
    "        'model_a_input_ids': model_a_encoding['input_ids'],\n",
    "        'model_a_attention_mask': model_a_encoding['attention_mask'],\n",
    "        'model_b_input_ids': model_b_encoding['input_ids'],\n",
    "        'model_b_attention_mask': model_b_encoding['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "def test_collate_fn(batch, tokenizer, max_length=512):\n",
    "    \"\"\"Custom collate function for test data\"\"\"\n",
    "    prompts = [item['prompt'] for item in batch]\n",
    "    model_a_responses = [item['model_a_response'] for item in batch]\n",
    "    model_b_responses = [item['model_b_response'] for item in batch]\n",
    "    \n",
    "    # Tokenize each input type\n",
    "    prompt_encoding = tokenizer(\n",
    "        prompts, padding=True, truncation=True, \n",
    "        max_length=max_length, return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    model_a_encoding = tokenizer(\n",
    "        model_a_responses, padding=True, truncation=True,\n",
    "        max_length=max_length, return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    model_b_encoding = tokenizer(\n",
    "        model_b_responses, padding=True, truncation=True,\n",
    "        max_length=max_length, return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'prompt_input_ids': prompt_encoding['input_ids'],\n",
    "        'prompt_attention_mask': prompt_encoding['attention_mask'],\n",
    "        'model_a_input_ids': model_a_encoding['input_ids'],\n",
    "        'model_a_attention_mask': model_a_encoding['attention_mask'],\n",
    "        'model_b_input_ids': model_b_encoding['input_ids'],\n",
    "        'model_b_attention_mask': model_b_encoding['attention_mask']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c48da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClassificationModel(pl.LightningModule):\n",
    "    \"\"\"PyTorch Lightning model for LLM Classification\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', learning_rate=5e-5, \n",
    "                 hidden_dim=256, num_classes=3, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Load BERT model - Download directly from HuggingFace\n",
    "        print(f\"🤖 Loading BERT model: {model_name}\")\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze most BERT parameters\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Classification head\n",
    "        bert_dim = self.bert.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, prompt_input_ids, prompt_attention_mask,\n",
    "                model_a_input_ids, model_a_attention_mask,\n",
    "                model_b_input_ids, model_b_attention_mask):\n",
    "        \n",
    "        # Get embeddings for each input\n",
    "        prompt_outputs = self.bert(input_ids=prompt_input_ids, attention_mask=prompt_attention_mask)\n",
    "        prompt_embedding = prompt_outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        model_a_outputs = self.bert(input_ids=model_a_input_ids, attention_mask=model_a_attention_mask)\n",
    "        model_a_embedding = model_a_outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        model_b_outputs = self.bert(input_ids=model_b_input_ids, attention_mask=model_b_attention_mask)\n",
    "        model_b_embedding = model_b_outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        concatenated = torch.cat([prompt_embedding, model_a_embedding, model_b_embedding], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(concatenated)\n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            batch['prompt_input_ids'], batch['prompt_attention_mask'],\n",
    "            batch['model_a_input_ids'], batch['model_a_attention_mask'],\n",
    "            batch['model_b_input_ids'], batch['model_b_attention_mask']\n",
    "        )\n",
    "        \n",
    "        loss = self.loss_fn(logits, batch['labels'])\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == batch['labels']).float().mean()\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self.forward(\n",
    "            batch['prompt_input_ids'], batch['prompt_attention_mask'],\n",
    "            batch['model_a_input_ids'], batch['model_a_attention_mask'],\n",
    "            batch['model_b_input_ids'], batch['model_b_attention_mask']\n",
    "        )\n",
    "        \n",
    "        loss = self.loss_fn(logits, batch['labels'])\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == batch['labels']).float().mean()\n",
    "        \n",
    "        # Logging\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return {'val_loss': loss, 'preds': preds, 'labels': batch['labels']}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d285e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Data module defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class LLMDataModule(pl.LightningDataModule):\n",
    "    \"\"\"PyTorch Lightning DataModule for LLM Classification\"\"\"\n",
    "    \n",
    "    def __init__(self, train_df, val_df, tokenizer, batch_size=16, max_length=512, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = LLMClassificationDataset(self.train_df, self.tokenizer, self.max_length)\n",
    "            self.val_dataset = LLMClassificationDataset(self.val_df, self.tokenizer, self.max_length)\n",
    "    \n",
    "    def _collate_fn(self, batch):\n",
    "        \"\"\"Wrapper for collate_fn that can be pickled\"\"\"\n",
    "        return collate_fn(batch, self.tokenizer, self.max_length)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,  \n",
    "            collate_fn=self._collate_fn,\n",
    "            pin_memory=False\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,  \n",
    "            collate_fn=self._collate_fn,\n",
    "            pin_memory=False\n",
    "        )\n",
    "\n",
    "print(\"Data module defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f8ef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Loading BERT model: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "data_module = LLMDataModule(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    max_length=config.MAX_LENGTH,\n",
    "    num_workers=config.NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = LLMClassificationModel(\n",
    "    model_name=config.MODEL_NAME,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    hidden_dim=config.HIDDEN_DIM,\n",
    "    num_classes=config.NUM_CLASSES,\n",
    "    dropout_rate=config.DROPOUT_RATE\n",
    ")\n",
    "\n",
    "# Set up callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='./checkpoints',\n",
    "    filename='llm-classification-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Set up logger\n",
    "logger = TensorBoardLogger('tb_logs', name='llm_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60420005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2025-10-14 20:33:28.740553: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with PyTorch Lightning...\n",
      "Using device: cuda:0\n",
      "Mixed precision: Enabled\n",
      "Training samples: 45981\n",
      "Validation samples: 11496\n",
      "Validation frequency: Once per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760454208.803371    3572 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760454208.823206    3572 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760454208.970753    3572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760454208.970777    3572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760454208.970778    3572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760454208.970779    3572 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-14 20:33:28.986768: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/obliviontrek/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name       | Type             | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | bert       | BertModel        | 109 M  | eval \n",
      "1 | classifier | Sequential       | 623 K  | train\n",
      "2 | loss_fn    | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------\n",
      "623 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "110 M     Total params\n",
      "440.422   Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obliviontrek/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 228 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2874/2874 [31:34<00:00,  1.52it/s, v_num=0, train_loss_step=0.915, train_acc_step=0.615, val_loss=1.040, val_acc=0.455, train_loss_epoch=1.040, train_acc_epoch=0.461] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2874/2874 [31:35<00:00,  1.52it/s, v_num=0, train_loss_step=0.915, train_acc_step=0.615, val_loss=1.040, val_acc=0.455, train_loss_epoch=1.040, train_acc_epoch=0.461]\n",
      "Training completed!\n",
      "Best model saved at: /home/obliviontrek/Documents/llm-finetuning-kaggle/checkpoints/llm-classification-epoch=01-val_acc=0.46.ckpt\n",
      "Best validation accuracy: 0.4554\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=config.MAX_EPOCHS,\n",
    "    precision='16-mixed',  # Enable mixed precision training\n",
    "    accelerator='auto',    # Automatically detect GPU/CPU\n",
    "    devices='auto',        # Use all available devices\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=50,\n",
    "    val_check_interval=1.0,  # Validate once per epoch\n",
    "    gradient_clip_val=1.0,   # Gradient clipping\n",
    "    accumulate_grad_batches=1,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(\"Starting training with PyTorch Lightning...\")\n",
    "print(f\"Using device: {trainer.strategy.root_device}\")\n",
    "print(f\"Mixed precision: {'Enabled' if trainer.precision == '16-mixed' else 'Disabled'}\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Validation frequency: Once per epoch\")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best model saved at: {checkpoint_callback.best_model_path}\")\n",
    "print(f\"Best validation accuracy: {checkpoint_callback.best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec6e23b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Test data shape: (3, 4)\n",
      "Test data columns: ['id', 'prompt', 'response_a', 'response_b']\n",
      "\n",
      "Sample test data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>[\"I have three oranges today, I ate an orange ...</td>\n",
       "      <td>[\"You have two oranges today.\"]</td>\n",
       "      <td>[\"You still have three oranges. Eating an oran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>[\"You are a mediator in a heated political deb...</td>\n",
       "      <td>[\"Thank you for sharing the details of the sit...</td>\n",
       "      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                             prompt  \\\n",
       "0  136060  [\"I have three oranges today, I ate an orange ...   \n",
       "1  211333  [\"You are a mediator in a heated political deb...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                    [\"You have two oranges today.\"]   \n",
       "1  [\"Thank you for sharing the details of the sit...   \n",
       "\n",
       "                                          response_b  \n",
       "0  [\"You still have three oranges. Eating an oran...  \n",
       "1  [\"Mr Reddy and Ms Blue both have valid points ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test_data(config):\n",
    "    \"\"\"Load test data for inference\"\"\"\n",
    "    test_file_path = config.DATA_PATH / 'test.csv'\n",
    "    if not test_file_path.exists():\n",
    "        raise FileNotFoundError(f\"Test data not found at {test_file_path}\")\n",
    "    \n",
    "    test_data = pd.read_csv(test_file_path)\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    print(f\"Test data columns: {test_data.columns.tolist()}\")\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_data = load_test_data(config)\n",
    "\n",
    "print(\"\\nSample test data:\")\n",
    "test_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d12715d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n",
      "🤖 Loading BERT model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obliviontrek/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability sums - Min: 1.0000, Max: 1.0000, Mean: 1.0000\n",
      "        id  winner_model_a  winner_model_b  winner_tie\n",
      "0   136060        0.376628        0.200569    0.422803\n",
      "1   211333        0.555219        0.177617    0.267163\n",
      "2  1233961        0.293212        0.365676    0.341112\n",
      "Submission file saved as 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "def run_inference(test_data, checkpoint_path, tokenizer, config):\n",
    "    \"\"\"Run inference on test data\"\"\"\n",
    "    print(\"Starting inference...\")\n",
    "    \n",
    "    # Load the best model checkpoint\n",
    "    best_model = LLMClassificationModel.load_from_checkpoint(checkpoint_path)\n",
    "    best_model.eval()\n",
    "    best_model.freeze()\n",
    "    \n",
    "    # Create test dataset and dataloader\n",
    "    test_dataset = TestDataset(test_data, tokenizer, max_length=config.MAX_LENGTH)\n",
    "    \n",
    "    def _test_collate_fn(batch):\n",
    "        \"\"\"Wrapper for test_collate_fn that can be pickled\"\"\"\n",
    "        return test_collate_fn(batch, tokenizer, max_length=config.MAX_LENGTH)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4, \n",
    "        collate_fn=_test_collate_fn,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Perform inference\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(best_model.device) for k, v in batch.items()}\n",
    "            \n",
    "            # Get logits\n",
    "            logits = best_model(\n",
    "                batch['prompt_input_ids'], batch['prompt_attention_mask'],\n",
    "                batch['model_a_input_ids'], batch['model_a_attention_mask'],\n",
    "                batch['model_b_input_ids'], batch['model_b_attention_mask']\n",
    "            )\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            all_predictions.append(probabilities.cpu())\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {batch_idx + 1}/{len(test_loader)} batches\")\n",
    "    \n",
    "    # Concatenate all predictions\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    \n",
    "    # Extract probabilities for each class\n",
    "    winner_model_a_probs = all_predictions[:, 0].numpy()  # Class 0: model_a wins\n",
    "    winner_model_b_probs = all_predictions[:, 1].numpy()  # Class 1: model_b wins\n",
    "    winner_tie_probs = all_predictions[:, 2].numpy()      # Class 2: tie\n",
    "    \n",
    "    # Create submission dataframe\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_data['id'],\n",
    "        'winner_model_a': winner_model_a_probs,\n",
    "        'winner_model_b': winner_model_b_probs,\n",
    "        'winner_tie': winner_tie_probs\n",
    "    })\n",
    "    \n",
    "    # Verify probabilities sum to 1\n",
    "    prob_sums = submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1)\n",
    "    print(f\"Probability sums - Min: {prob_sums.min():.4f}, Max: {prob_sums.max():.4f}, Mean: {prob_sums.mean():.4f}\")\n",
    "    \n",
    "    print(submission.head())\n",
    "    \n",
    "    # Save submission as requested filename\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission file saved as 'submission.csv'\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# Run inference and create submission\n",
    "submission = run_inference(test_data, checkpoint_callback.best_model_path, tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6adce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
